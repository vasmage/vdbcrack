{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sos:\n",
    "- Can not use Vstack or Hstack invlist objects, they are read only they can not be copied, you can not assign stuff to them etc etc..\n",
    "- thus this code now is wrong, can't use it...\n",
    "\n",
    "## I think:\n",
    "- FAISS does not support resizing an inverted list to be larger. I don't know how to go from nlist = 10, to nlist = 11 ( crack one point) without doing .add_preassigned() again from scratch (which is not what we want...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I want the prototype (python) implementation to be?\n",
    "- start w/ knn()\n",
    "- cracking will slow down brute force search\n",
    "    - implemented as vstack invlists and need re-assignments\n",
    "- refine:\n",
    "    - inner kmeans, \n",
    "    - get points visited/refine is the most expensive operation ( has copy O(pts_copied) not O(invlists copied))\n",
    "- reorg:\n",
    "    - I think this is not as expensive\n",
    "\n",
    "\n",
    "In the python prototype, the logic of the algorithm should be correct and we should be able to see if the general idea holds. But there are 0 optimizations and cracking is slow.\n",
    "\n",
    "\n",
    "Todo:\n",
    "- need to think of a way to estimate cost of .add() for 1 or multiple centroids and depending if it's global/local and how many points etc etc....\n",
    "- eg. cracking by adding 100 new centroids vs just 1 is just as expensive for 1M points, so obviously one is a better choice etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching:\n",
    "- if you batch it, it is much faster than if you go 1 query at a time\n",
    "IVFFlat\n",
    "    - ~20ms per query -> ~200 sec for 10k queries 3:20 secs: --> but if you batch them all together ~32 secs @16 threads\n",
    "- batch also helps, but not as huge of a difference as in Brute Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import faiss\n",
    "from vasili_helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sift1M...done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:\n",
      "xb.shape=(1000000, 128)\n",
      "gt.shape=(10000, 100)\n",
      "xq.shape=(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "xb, xq, xt, gt, metric = load_sift1M(f\"/pub/scratch/vmageirakos/datasets/ann-fvecs/sift-128-euclidean\")\n",
    "nb, d = xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test cracking a larger index..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1000 km_n_iter=10 km_max_pts=256 seed=1 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 1000000 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "IndexIVFFlat::add_core: added 1000000 / 1000000 vectors\n",
      "\t---> Index Train Time = 1012.9538299952401 ms | Add Time = 385.0520280029741 ms <---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vasili_helpers import *\n",
    "\n",
    "index_to_crack, train_time, add_time = train_ivfflat(\n",
    "    xb,\n",
    "    nlist=1000,\n",
    "    km_n_iter=10,\n",
    "    km_max_pts=256,\n",
    "    seed=1,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    "    store=False,\n",
    ")\n",
    "\n",
    "index_to_crack.invlists.nlist\n",
    "\n",
    "# index_to_crack.own_invlists = False # ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INVALID_CODE_SIZE',\n",
       " 'SUBSET_TYPE_ELEMENT_RANGE',\n",
       " 'SUBSET_TYPE_ID_MOD',\n",
       " 'SUBSET_TYPE_ID_RANGE',\n",
       " 'SUBSET_TYPE_INVLIST',\n",
       " 'SUBSET_TYPE_INVLIST_FRACTION',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__swig_destroy__',\n",
       " '__weakref__',\n",
       " 'add_entries',\n",
       " 'add_entry',\n",
       " 'code_size',\n",
       " 'compute_ntotal',\n",
       " 'copy_subset_to',\n",
       " 'get_codes',\n",
       " 'get_ids',\n",
       " 'get_iterator',\n",
       " 'get_single_code',\n",
       " 'get_single_id',\n",
       " 'imbalance_factor',\n",
       " 'is_empty',\n",
       " 'list_size',\n",
       " 'merge_from',\n",
       " 'nlist',\n",
       " 'prefetch_lists',\n",
       " 'print_stats',\n",
       " 'release_codes',\n",
       " 'release_ids',\n",
       " 'reset',\n",
       " 'resize',\n",
       " 'this',\n",
       " 'thisown',\n",
       " 'update_entries',\n",
       " 'update_entry',\n",
       " 'use_iterator']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(index_to_crack.invlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.InvertedLists; proxy of <Swig Object of type 'faiss::InvertedLists *' at 0x7f3da1f85a70> >"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.ArrayInvertedLists; proxy of <Swig Object of type 'faiss::ArrayInvertedLists *' at 0x7f3da1e1aeb0> >"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack.invlists\n",
    "faiss.downcast_InvertedLists(index_to_crack.invlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1000 km_n_iter=10 km_max_pts=256 seed=1 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 1000000 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "IndexIVFFlat::add_core: added 1000000 / 1000000 vectors\n",
      "\t---> Index Train Time = 965.3408289959771 ms | Add Time = 387.23051799752284 ms <---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7f3da1f7ec70> >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- combined_invlists=<faiss.swigfaiss_avx2.VStackInvertedLists; proxy of <Swig Object of type 'faiss::VStackInvertedLists *' at 0x7f3da1f7f0c0> > ---\n",
      "--- combined_invlists=<faiss.swigfaiss_avx2.VStackInvertedLists; proxy of <Swig Object of type 'faiss::VStackInvertedLists *' at 0x7f3da1f08de0> > ---\n",
      "\n",
      "1) combined_index.invlists.nlist=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1 points to 1 centroids: please provide at least 39 training points\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in void faiss::IndexIVF::replace_invlists(faiss::InvertedLists*, bool) at /home/runner/miniconda3/conda-bld/faiss-pkg_1728491153420/work/faiss/IndexIVF.cpp:1236: Error: 'il->nlist == nlist' failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_257097/3842882493.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mindex_to_crack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mindex_to_crack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mown_invlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# ????\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m blah = crack_single(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mindex_to_crack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_to_crack\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcrack_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0massignments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/crack-vdb/src/main_code/vasili_helpers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(index_to_crack, crack_loc, assignments, metric)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m1) \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcombined_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcombined_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_trained\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# NOTE: true or fals for .own_invlists() ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mcombined_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_invlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_invlists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BUG: not correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m2) \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcombined_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcombined_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlist\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcombined_nlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcombined_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/enviroments/miniforge3/envs/faiss-dev-env/lib/python3.12/site-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, il, own)\u001b[0m\n\u001b[1;32m   6244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplace_invlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6245\u001b[0m         \u001b[0;34mr\"\"\" replace the inverted lists, old one is deallocated if own_invlists\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6246\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexIVF_replace_invlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in void faiss::IndexIVF::replace_invlists(faiss::InvertedLists*, bool) at /home/runner/miniconda3/conda-bld/faiss-pkg_1728491153420/work/faiss/IndexIVF.cpp:1236: Error: 'il->nlist == nlist' failed"
     ]
    }
   ],
   "source": [
    "from vasili_helpers import *\n",
    "\n",
    "index_to_crack, train_time, add_time = train_ivfflat(\n",
    "    xb,\n",
    "    nlist=1000,\n",
    "    km_n_iter=10,\n",
    "    km_max_pts=256,\n",
    "    seed=1,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    "    store=False,\n",
    ")\n",
    "\n",
    "index_to_crack\n",
    "index_to_crack.own_invlists = False # ????\n",
    "\n",
    "blah = crack_single(\n",
    "    index_to_crack=index_to_crack, \n",
    "    crack_loc=xq[0:1, :], \n",
    "    assignments=None, \n",
    "    metric=metric\n",
    "    )\n",
    "\n",
    "index_to_crack.nlist\n",
    "index_to_crack.invlists.nlist\n",
    "index_to_crack.nprobe=1\n",
    "# index_to_crack.search(xq[0:1,:], k=10)\n",
    "\n",
    "print(f\"5) {index_to_crack.invlists.nlist=}\")    \n",
    "print(f\"5) {blah.invlists.nlist=}\")    \n",
    "print(f\"5) {blah=}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[55091., 59531., 69844., 73793., 74016., 75124., 75554., 78092.,\n",
       "         78889., 79508.]], dtype=float32),\n",
       " array([[934876, 561813, 701258, 600499, 893601, 562167, 746931, 779712,\n",
       "          49874, 701919]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([[55091., 59531., 69844., 73793., 74016., 75124., 75554., 78092.,\n",
       "         78889., 79508.]], dtype=float32),\n",
       " array([[934876, 561813, 701258, 600499, 893601, 562167, 746931, 779712,\n",
       "          49874, 701919]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack.search(xq[0:1, :], 10)\n",
    "index_to_crack.d\n",
    "index_to_crack.search(xq[0:1, :], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "index_to_crackindex_to_crack.invlists.list_size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fcf83a3cae0> >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack\n",
    "index_to_crack.nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an index with centroid being the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an Inverted Lists object with a single inverted list being the new centroid we wish to grow from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the index with a single inverted list, defined by centroid being the first point in the dataset, and from then on you can \"crack\" based on the queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: is there a faster way to do the .add()? just add xb into the 1st invlist, instead of doing distance computations to .add() which I think is what's happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss.omp_set_num_threads(127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "- add_preassigned I think calls `void IndexIVF::add_core` which parallelizes over inv lists and loops over points. ----> So O(#points) complexity in the .add() \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_point = xb[0:1, :]\n",
    "# quantizer = faiss.IndexFlatL2(d)\n",
    "# TEST_idx = faiss.IndexIVFFlat(quantizer, d, 1, faiss.METRIC_L2)\n",
    "\n",
    "# strain = time.perf_counter()\n",
    "# TEST_idx.train(first_point)\n",
    "# etrain = time.perf_counter()\n",
    "\n",
    "# sadd = time.perf_counter()\n",
    "# # crack_idx.add(xb) # this has some cost of a few queries through brute force\n",
    "# eadd = time.perf_counter()\n",
    "\n",
    "# TEST_idx.ntotal\n",
    "# TEST_idx.nlist\n",
    "# TEST_idx.invlists.imbalance_factor()\n",
    "# TEST_idx.invlists.print_stats()\n",
    "# TEST_idx.invlists.list_size(0)\n",
    "# TEST_idx.is_trained\n",
    "# print(f\"\\t---> Index Train Time = {(etrain - strain)*1000} ms | Add Time = {(eadd - sadd)*1000} ms <---\")\n",
    "\n",
    "# # ## add preassigned\n",
    "# assignements = np.zeros(xb.shape[0], dtype='int64')\n",
    "# from faiss.contrib.ivf_tools import add_preassigned\n",
    "# # # you need to set this to\n",
    "# # TEST_idx.is_trained = True\n",
    "\n",
    "# start = time.perf_counter()\n",
    "# add_preassigned(index_ivf=TEST_idx, x=xb, a=assignements)#, ids=xb_ids) # ids implied by order of pts\n",
    "# end = time.perf_counter()\n",
    "\n",
    "# TEST_idx.ntotal\n",
    "# TEST_idx.nlist\n",
    "# TEST_idx.invlists.imbalance_factor()\n",
    "# TEST_idx.invlists.print_stats()\n",
    "# TEST_idx.invlists.list_size(0)\n",
    "\n",
    "# # it still takes ~600ms to add to invlist, so it's quite slow\n",
    "# # adding to 1 invlist or ~100 for sift is about the same cost... <<< how to model this & why\n",
    "# print(f\"\\t---> Add preassigned = {(end - start)*1000} ms <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: should I try to add entries to add batch in inv list?\n",
    "``` cpp\n",
    "size_t ArrayInvertedLists::add_entries(\n",
    "        size_t list_no,\n",
    "        size_t n_entry,\n",
    "        const idx_t* ids_in,\n",
    "        const uint8_t* code) {\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # brute force serach specific invlist \n",
    "# from faiss.contrib.ivf_tools import search_preassigned\n",
    "# TEST_idx.nprobe = 1\n",
    "# search_preassigned(TEST_idx, xq, k=100, list_nos=np.zeros(xq.shape[0], dtype='int64').reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crack (default/naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index_to_crack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "first_point = xb[0:1, :]\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "if metric == \"euclidean\":\n",
    "    index_to_crack = faiss.IndexIVFFlat(quantizer, d, 1, faiss.METRIC_L2)\n",
    "elif metric == \"angular\":\n",
    "    index_to_crack = faiss.IndexIVFFlat(quantizer, d, 1, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "\n",
    "strain = time.perf_counter()\n",
    "index_to_crack.train(first_point)\n",
    "etrain = time.perf_counter()\n",
    "\n",
    "sadd = time.perf_counter()\n",
    "index_to_crack.add(xb) # this has some cost of a few queries through brute force\n",
    "eadd = time.perf_counter()\n",
    "\n",
    "index_to_crack.ntotal\n",
    "index_to_crack.nlist\n",
    "index_to_crack.invlists.imbalance_factor()\n",
    "index_to_crack.invlists.print_stats()\n",
    "index_to_crack.invlists.list_size(0)\n",
    "index_to_crack\n",
    "\n",
    "print(f\"\\t---> Index Train Time = {(etrain - strain)*1000} ms | Add Time = {(eadd - sadd)*1000} ms <---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vasili_helpers import *\n",
    "\n",
    "index_to_crack = crack_single(\n",
    "    index_to_crack=index_to_crack, \n",
    "    crack_loc=xq[0:1, :], \n",
    "    assignments=None, \n",
    "    metric=metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- After crack, index_to_crack.nlist=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1 points to 1 centroids: please provide at least 39 training points\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From then on, for incoming queries you can decide to \"crack\" which means adding more centroids and corresponding invlists (partitions) to the index_to_crack\n",
    "- if it's the very first one, we have to add all points to the same invlist, as we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1 points to 1 centroids: please provide at least 39 training points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t---> Index Train Time = 0.08796999463811517 ms | Add Time = 0.01172000338556245 ms <---\n"
     ]
    }
   ],
   "source": [
    "crack_query = xq[0:1, :]\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "if metric == \"euclidean\":\n",
    "    new_crack = faiss.IndexIVFFlat(quantizer, d, 1, faiss.METRIC_L2)\n",
    "elif metric == \"angular\":\n",
    "    new_crack = faiss.IndexIVFFlat(quantizer, d, 1, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "\n",
    "strain = time.perf_counter()\n",
    "new_crack.train(crack_query)\n",
    "etrain = time.perf_counter()\n",
    "\n",
    "# NOTE: if you know the assignments at this point, you can add the points here to the invlist\n",
    "#       - but make sure you've removed them from the other invlists (no copies)\n",
    "sadd = time.perf_counter()\n",
    "# new_crack.add()\n",
    "eadd = time.perf_counter()\n",
    "\n",
    "new_crack.ntotal\n",
    "new_crack.nlist\n",
    "\n",
    "print(f\"\\t---> Index Train Time = {(etrain - strain)*1000} ms | Add Time = {(eadd - sadd)*1000} ms <---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to merge the invlist of the running query to the cracked query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilv = faiss.InvertedListsPtrVector() # a pointer for multiple invlists\n",
    "# existing invlists\n",
    "existing_invlists = index_to_crack.invlists # \n",
    "ilv.push_back(existing_invlists)\n",
    "# adding the new one (crack)\n",
    "new_invlist = new_crack.invlists\n",
    "ilv.push_back(new_invlist)\n",
    "combined_invlists = faiss.VStackInvertedLists(ilv.size(), ilv.data())\n",
    "# big_il = faiss.HStackInvertedLists(ilv.size(), ilv.data())\n",
    "combined_invlists.nlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure that all points are still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.VStackInvertedLists; proxy of <Swig Object of type 'faiss::VStackInvertedLists *' at 0x7f72407aa6d0> >"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_invlists\n",
    "ntotal_new = combined_invlists.compute_ntotal()\n",
    "ntotal_new  # take care not to have query in xb of new index\n",
    "assert ntotal_new == index_to_crack.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the centroids which at this point you already have no need to reconstruct, but as an exmple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_centroids = np.vstack([index_to_crack.quantizer.reconstruct_n(), new_crack.quantizer.reconstruct_n()])\n",
    "combined_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_nlist = combined_invlists.nlist\n",
    "combined_nlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new empty index with the correct centroids and nlist size, to merge the above two. Don't think it's possible to merge in-place in index_to_crack, so we'll replace it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 82 μs, total: 82 μs\n",
      "Wall time: 68.2 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_quantizer = faiss.IndexFlatL2(d)\n",
    "combined_quantizer.add(combined_centroids)\n",
    "# new empty index\n",
    "combined_index = faiss.IndexIVFFlat(combined_quantizer, d, combined_nlist, faiss.METRIC_L2)\n",
    "\n",
    "# combined_index.ntotal\n",
    "# combined_index.is_trained\n",
    "# combined_index.nlist\n",
    "# combined_index.invlists.nlist\n",
    "# combined_index.invlists.print_stats() # empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I don' tknow if own_invlists should be True or False at this point, which is set in replace_invlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 μs, sys: 4 μs, total: 29 μs\n",
      "Wall time: 35.5 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_index.replace_invlists(combined_invlists, True) # True/False is wether own_invlists ( ie if index is deleted this arrayinvlists is deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_index.own_invlists\n",
    "# combined_index.ntotal\n",
    "# combined_index.is_trained\n",
    "# combined_index.nlist\n",
    "# combined_index.invlists.nlist\n",
    "# combined_index.invlists.print_stats() # empty\n",
    "# combined_index.ntotal # why 0?\n",
    "# combined_index.own_invlists # <<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to manually set the ntotal because it's not tracked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_index.ntotal = combined_invlists.compute_ntotal()\n",
    "combined_index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we created it now, all points are in one invlist, and the other is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_index.invlists.list_size(0) # original one\n",
    "combined_index.invlists.list_size(1) # cracked one (we didn't add any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you obviously also need to handle the assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you query nprobe = 1 you should get recall < 1   \n",
    "if you query nprobe = 2 you should get recall = 2 ( because like above it's brute force )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[932085, 934876, 561813, ..., 398306, 931721, 989762],\n",
       "       [413247, 413071, 706838, ..., 855176, 846198, 987074],\n",
       "       [669835, 408764, 408462, ..., 310475, 971815, 937903],\n",
       "       ...,\n",
       "       [123855, 123351, 534149, ...,  90175, 685486, 416474],\n",
       "       [755327, 755323, 840765, ..., 595134, 601257, 172180],\n",
       "       [874343, 464509, 413340, ..., 360985, 419949, 223427]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.510959"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_index.nprobe = 1\n",
    "a, b = combined_index.search(xq, k=100)\n",
    "gt\n",
    "compute_recall(b, gt, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[932085, 934876, 561813, ..., 398306, 931721, 989762],\n",
       "       [413247, 413071, 706838, ..., 855176, 846198, 987074],\n",
       "       [669835, 408764, 408462, ..., 310475, 971815, 937903],\n",
       "       ...,\n",
       "       [123855, 123351, 534149, ...,  90175, 685486, 416474],\n",
       "       [755327, 755323, 840765, ..., 595134, 601257, 172180],\n",
       "       [874343, 464509, 413340, ..., 360985, 419949, 223427]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.999889"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_index.nprobe = 2\n",
    "a, b = combined_index.search(xq, k=100)\n",
    "gt\n",
    "compute_recall(b, gt, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much slower is this than brute force?\n",
    "- invlist scanner is much slower than brute force in current faiss implementation\n",
    "    - there's the overhead for finding nprobe nearest centroids, but still it's not fast\n",
    "    - in theory we can speed it up greatly but not an issue for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.omp_set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.93 s, sys: 3.82 ms, total: 3.94 s\n",
      "Wall time: 3.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "index_to_crack.nprobe = 1\n",
    "# a, b = index_to_crack.search(xq, k=100)\n",
    "a, b = index_to_crack.search(xq[:num_queries,:], k=100)\n",
    "compute_recall(b, gt, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.18 s, sys: 71 μs, total: 3.18 s\n",
      "Wall time: 3.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined_index.nprobe = 2\n",
    "# a, b = combined_index.search(xq, k=100)\n",
    "a, b = combined_index.search(xq[:num_queries,:], k=100)\n",
    "compute_recall(b, gt, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 385 ms, sys: 11.7 ms, total: 397 ms\n",
      "Wall time: 398 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from faiss.contrib.exhaustive_search import knn\n",
    "# _, _ = knn(xq, xb, 100)\n",
    "_, _ = knn(xq[:num_queries,:], xb, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to update, such that index_to_crack is the new combined_idx and we don't take extra space...\n",
    "- will the invlists be deleted because own_invlists=true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fc4ee9a7c30> >"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fc946c17f60> >"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fc4ee7f9da0> >"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 3 indexes at this point, we should only move forward with index_to_crack\n",
    "index_to_crack\n",
    "new_crack\n",
    "combined_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fc4ee7f9da0> >"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fc946c17f60> >"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fc4ee7f9da0> >"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del index_to_crack\n",
    "index_to_crack = combined_index\n",
    "index_to_crack\n",
    "new_crack\n",
    "combined_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crack_idx.nlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (skip) Compare .add() times for different num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.omp_set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1 km_n_iter=0 km_max_pts=256 seed=42 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 1000000 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "\t---> Index Train Time = 48.224473001027945 ms | Add Time = 392.23024599778 ms <---IndexIVFFlat::add_core: added 1000000 / 1000000 vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlist = 1\n",
    "n_iter = 0\n",
    "max_pts = 256\n",
    "seed = 42 \n",
    "result_dir= None\n",
    "\n",
    "index, train_time, add_time = train_ivfflat(\n",
    "    xb,\n",
    "    nlist=nlist,\n",
    "    km_n_iter=n_iter,\n",
    "    km_max_pts=max_pts,\n",
    "    seed=seed,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:\n",
      "xb.shape=(10000000, 128)\n",
      "xq.shape=(1, 128)\n"
     ]
    }
   ],
   "source": [
    "a, b = create_random_dataset_fast(d=128, nt=1, nb=10000000, nq=1, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1 km_n_iter=0 km_max_pts=256 seed=42 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 10000000 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "\t---> Index Train Time = 539.1174030010006 ms | Add Time = 7559.182643002714 ms <---IndexIVFFlat::add_core: added 10000000 / 10000000 vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlist = 1\n",
    "n_iter = 0\n",
    "max_pts = 256\n",
    "seed = 42 \n",
    "result_dir= None\n",
    "\n",
    "index, train_time, add_time = train_ivfflat(\n",
    "    a,\n",
    "    nlist=nlist,\n",
    "    km_n_iter=n_iter,\n",
    "    km_max_pts=max_pts,\n",
    "    seed=seed,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1000 km_n_iter=0 km_max_pts=256 seed=42 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 10000000 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "\t---> Index Train Time = 600.1265039994905 ms | Add Time = 27258.726158997888 ms <---IndexIVFFlat::add_core: added 10000000 / 10000000 vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlist = 1000\n",
    "n_iter = 0\n",
    "max_pts = 256\n",
    "seed = 42 \n",
    "result_dir= None\n",
    "\n",
    "index, train_time, add_time = train_ivfflat(\n",
    "    a,\n",
    "    nlist=nlist,\n",
    "    km_n_iter=n_iter,\n",
    "    km_max_pts=max_pts,\n",
    "    seed=seed,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (skip) Brute force, knn vs indexflat\n",
    "- can just use knn() \n",
    "- or create IndexFlat() ( but has small add time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bf_index = faiss.IndexFlatL2(d)\n",
    "# bf_index.add(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _, _ = bf_index.search(xq, k=100) # if you just run it what is the time showing vs what if I run it with my method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# bf_index.search(xq[0:1,:], k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# nq, d = xq.shape\n",
    "# batch_size = 1\n",
    "# latency_per_batch=[]\n",
    "# topK = 100\n",
    "# I = np.empty((nq, topK), dtype=\"int32\")\n",
    "# D = np.empty((nq, topK), dtype=\"float32\")\n",
    "# i0 = 0\n",
    "# t00 = time.perf_counter()\n",
    "# while i0 < nq:\n",
    "#     t_batch_start = time.perf_counter()\n",
    "\n",
    "#     if i0 + batch_size < nq:\n",
    "#         i1 = i0 + batch_size\n",
    "#     else:\n",
    "#         i1 = nq\n",
    "    \n",
    "#     Di, Ii = bf_index.search(xq[i0:i1], topK)\n",
    "\n",
    "#     I[i0:i1] = Ii\n",
    "#     D[i0:i1] = Di\n",
    "\n",
    "#     t_batch_end = time.perf_counter()\n",
    "#     latency_per_batch.append(t_batch_end - t_batch_start)\n",
    "\n",
    "#     i0 = i1 # prepare for next iter\n",
    "\n",
    "# latency_ms_per_batch_this_run = np.array(latency_per_batch) * 1000\n",
    "# if (\n",
    "#     len(latency_ms_per_batch_this_run) > 2\n",
    "# ):  # remove first and last batch latency\n",
    "#     latency_ms_per_batch_this_run = latency_ms_per_batch_this_run[\n",
    "#         1:-1\n",
    "#     ]\n",
    "# print(\n",
    "#     \"Median batch latency: {:.3f} ms\".format(\n",
    "#         np.median(latency_ms_per_batch_this_run)\n",
    "#     ) + \" | \" +\n",
    "#     \"Mean batch latency: {:.3f} ms\".format(\n",
    "#         np.mean(latency_ms_per_batch_this_run)\n",
    "#     ) + \" | \" +\n",
    "#     \"99th percentile batch latency: {:.3f} ms\".format(\n",
    "#         np.percentile(latency_ms_per_batch_this_run, 99)\n",
    "#     )\n",
    "# )\n",
    "# print(f\"Total time: {(time.perf_counter() - t00) *1000 } ms\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlist = 1000\n",
    "# n_iter = 0\n",
    "# max_pts = 256\n",
    "# seed =42 \n",
    "# result_dir= None\n",
    "\n",
    "# index, train_time, add_time = train_ivfflat(\n",
    "#     xb,\n",
    "#     nlist=nlist,\n",
    "#     km_n_iter=n_iter,\n",
    "#     km_max_pts=max_pts,\n",
    "#     seed=seed,\n",
    "#     store_dir=None,  # if you want to store the index\n",
    "#     verbose=True,\n",
    "#     metric=metric,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nq, d = xq.shape\n",
    "# batch_size = 16\n",
    "# latency_per_batch=[]\n",
    "# topK = 100\n",
    "# I = np.empty((nq, topK), dtype=\"int32\")\n",
    "# D = np.empty((nq, topK), dtype=\"float32\")\n",
    "# i0 = 0\n",
    "# while i0 < nq:\n",
    "#     t_batch_start = time.perf_counter()\n",
    "\n",
    "#     if i0 + batch_size < nq:\n",
    "#         i1 = i0 + batch_size\n",
    "#     else:\n",
    "#         i1 = nq\n",
    "    \n",
    "#     Di, Ii = index.search(xq[i0:i1], topK)\n",
    "\n",
    "#     I[i0:i1] = Ii\n",
    "#     D[i0:i1] = Di\n",
    "\n",
    "#     t_batch_end = time.perf_counter()\n",
    "#     latency_per_batch.append(t_batch_end - t_batch_start)\n",
    "\n",
    "#     i0 = i1 # prepare for next iter\n",
    "\n",
    "# latency_ms_per_batch_this_run = np.array(latency_per_batch) * 1000\n",
    "# if (\n",
    "#     len(latency_ms_per_batch_this_run) > 2\n",
    "# ):  # remove first and last batch latency\n",
    "#     latency_ms_per_batch_this_run = latency_ms_per_batch_this_run[\n",
    "#         1:-1\n",
    "#     ]\n",
    "# print(\n",
    "#     \"Median batch latency: {:.3f} ms\".format(\n",
    "#         np.median(latency_ms_per_batch_this_run)\n",
    "#     ) + \" | \" +\n",
    "#     \"Mean batch latency: {:.3f} ms\".format(\n",
    "#         np.mean(latency_ms_per_batch_this_run)\n",
    "#     ) + \" | \" +\n",
    "#     \"99th percentile batch latency: {:.3f} ms\".format(\n",
    "#         np.percentile(latency_ms_per_batch_this_run, 99)\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i0 = 0\n",
    "# i1 = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss.omp_set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Di, Ii = bf_index.search(xq[0:1], topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Di, Ii = bf_index.search(xq[0:i1], topK)\n",
    "# i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# batch_size = 1\n",
    "# i0 = 0\n",
    "# while i0 < nq:\n",
    "#     t_batch_start = time.perf_counter()\n",
    "\n",
    "#     if i0 + batch_size < nq:\n",
    "#         i1 = i0 + batch_size\n",
    "#     else:\n",
    "#         i1 = nq\n",
    "    \n",
    "#     Di, Ii = bf_index.search(xq[i0:i1], topK)\n",
    "\n",
    "#     I[i0:i1] = Ii\n",
    "#     D[i0:i1] = Di\n",
    "\n",
    "#     t_batch_end = time.perf_counter()\n",
    "#     latency_per_batch.append(t_batch_end - t_batch_start)\n",
    "\n",
    "#     i0 = i1 # prepare for next iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faiss.contrib.exhaustive_search import knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "# _, _ = knn(xq, xb, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# _, _ = bf_index.search(xq[199:200,:], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# _, _ = knn(xq[199:200,:], xb, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i0, i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Di, Ii = index.search(xq[0:i1], topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# batch_size = 1\n",
    "# i0 = 0\n",
    "# while i0 < nq:\n",
    "#     t_batch_start = time.perf_counter()\n",
    "\n",
    "#     if i0 + batch_size < nq:\n",
    "#         i1 = i0 + batch_size\n",
    "#     else:\n",
    "#         i1 = nq\n",
    "    \n",
    "#     Di, Ii = index.search(xq[i0:i1], topK)\n",
    "\n",
    "#     I[i0:i1] = Ii\n",
    "#     D[i0:i1] = Di\n",
    "\n",
    "#     t_batch_end = time.perf_counter()\n",
    "#     latency_per_batch.append(t_batch_end - t_batch_start)\n",
    "\n",
    "#     i0 = i1 # prepare for next iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
