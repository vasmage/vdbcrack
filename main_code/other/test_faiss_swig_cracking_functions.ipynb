{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is test code to figure out a minimal implementation using providede faiss functions to crack through python.\n",
    "This is to be used for the prototype implementation.\n",
    "The optimized implementation should be native C++ in faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import faiss\n",
    "from vasili_helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sift1M..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:\n",
      "xb.shape=(1000000, 128)\n",
      "gt.shape=(10000, 100)\n",
      "xq.shape=(10000, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "xb, xq, xt, gt, metric = load_sift1M(f\"/pub/scratch/vmageirakos/datasets/ann-fvecs/sift-128-euclidean\")\n",
    "nb, d = xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 1000\n",
    "n_iter = 0\n",
    "max_pts = 256\n",
    "seed =42 \n",
    "result_dir= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1000 km_n_iter=10 km_max_pts=256 seed=42 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 1000000 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "IndexIVFFlat::add_core: added 1000000 / 1000000 vectors\n",
      "\t---> Index Train Time = 997.8557150006964 ms | Add Time = 294.3818399999145 ms <---\n"
     ]
    }
   ],
   "source": [
    "index, train_time, add_time = train_ivfflat(\n",
    "    xb,\n",
    "    nlist=nlist,\n",
    "    km_n_iter=n_iter,\n",
    "    km_max_pts=max_pts,\n",
    "    seed=seed,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.own_invlists # if the index owns the InvertedList object (and can delete it etc. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INVALID_CODE_SIZE',\n",
       " 'SUBSET_TYPE_ELEMENT_RANGE',\n",
       " 'SUBSET_TYPE_ID_MOD',\n",
       " 'SUBSET_TYPE_ID_RANGE',\n",
       " 'SUBSET_TYPE_INVLIST',\n",
       " 'SUBSET_TYPE_INVLIST_FRACTION',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__swig_destroy__',\n",
       " '__weakref__',\n",
       " 'add_entries',\n",
       " 'add_entry',\n",
       " 'code_size',\n",
       " 'compute_ntotal',\n",
       " 'copy_subset_to',\n",
       " 'get_codes',\n",
       " 'get_ids',\n",
       " 'get_iterator',\n",
       " 'get_single_code',\n",
       " 'get_single_id',\n",
       " 'imbalance_factor',\n",
       " 'is_empty',\n",
       " 'list_size',\n",
       " 'merge_from',\n",
       " 'nlist',\n",
       " 'prefetch_lists',\n",
       " 'print_stats',\n",
       " 'release_codes',\n",
       " 'release_ids',\n",
       " 'reset',\n",
       " 'resize',\n",
       " 'this',\n",
       " 'thisown',\n",
       " 'update_entries',\n",
       " 'update_entry',\n",
       " 'use_iterator']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(index.invlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_crack = faiss.clone_index(index) # each run should start from same point because crack is in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ivfflat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index, train_time, add_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ivfflat\u001b[49m(\n\u001b[1;32m      2\u001b[0m     xb,\n\u001b[1;32m      3\u001b[0m     nlist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m     km_n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[1;32m      5\u001b[0m     km_max_pts\u001b[38;5;241m=\u001b[39mmax_pts,\n\u001b[1;32m      6\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m      7\u001b[0m     store_dir\u001b[38;5;241m=\u001b[39mresult_dir,  \u001b[38;5;66;03m# if you want to store the index\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ivfflat' is not defined"
     ]
    }
   ],
   "source": [
    "index, train_time, add_time = train_ivfflat(\n",
    "    xb,\n",
    "    nlist=1,\n",
    "    km_n_iter=n_iter,\n",
    "    km_max_pts=max_pts,\n",
    "    seed=seed,\n",
    "    store_dir=result_dir,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you increase number of centroids and try to replace quantizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_centroids = index_to_crack.quantizer.reconstruct_n() # or kmeans.centroids idk which is faster\n",
    "outer_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 more centroid\n",
    "first_query = xq[0:1, :]\n",
    "new_centroids = np.vstack([outer_centroids, first_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     new_quantizer \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(d)\n\u001b[1;32m      5\u001b[0m new_quantizer\u001b[38;5;241m.\u001b[39madd(new_centroids)\n\u001b[0;32m----> 6\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_ivf_quantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_to_crack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_quantizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# replace quantizer\u001b[39;00m\n",
      "File \u001b[0;32m~/enviroments/miniforge3/envs/faiss-dev-env/lib/python3.12/site-packages/faiss/contrib/ivf_tools.py:106\u001b[0m, in \u001b[0;36mreplace_ivf_quantizer\u001b[0;34m(index_ivf, new_quantizer)\u001b[0m\n\u001b[1;32m    104\u001b[0m     new_quantizer\u001b[38;5;241m.\u001b[39madd(centroids)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m new_quantizer\u001b[38;5;241m.\u001b[39mntotal \u001b[38;5;241m==\u001b[39m index_ivf\u001b[38;5;241m.\u001b[39mnlist\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# cleanly dealloc old quantizer\u001b[39;00m\n\u001b[1;32m    109\u001b[0m old_own \u001b[38;5;241m=\u001b[39m index_ivf\u001b[38;5;241m.\u001b[39mown_fields\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if metric == \"euclidean\":\n",
    "    new_quantizer = faiss.IndexFlatL2(d)\n",
    "elif metric == \"angular\":\n",
    "    new_quantizer = faiss.IndexFlatIP(d)\n",
    "new_quantizer.add(new_centroids)\n",
    "_ = replace_ivf_quantizer(index_to_crack, new_quantizer) # replace quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INVALID_CODE_SIZE',\n",
       " 'SUBSET_TYPE_ELEMENT_RANGE',\n",
       " 'SUBSET_TYPE_ID_MOD',\n",
       " 'SUBSET_TYPE_ID_RANGE',\n",
       " 'SUBSET_TYPE_INVLIST',\n",
       " 'SUBSET_TYPE_INVLIST_FRACTION',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__swig_destroy__',\n",
       " '__weakref__',\n",
       " 'add_entries',\n",
       " 'add_entry',\n",
       " 'code_size',\n",
       " 'compute_ntotal',\n",
       " 'copy_subset_to',\n",
       " 'get_codes',\n",
       " 'get_ids',\n",
       " 'get_iterator',\n",
       " 'get_single_code',\n",
       " 'get_single_id',\n",
       " 'imbalance_factor',\n",
       " 'is_empty',\n",
       " 'list_size',\n",
       " 'merge_from',\n",
       " 'nlist',\n",
       " 'prefetch_lists',\n",
       " 'print_stats',\n",
       " 'release_codes',\n",
       " 'release_ids',\n",
       " 'reset',\n",
       " 'resize',\n",
       " 'this',\n",
       " 'thisown',\n",
       " 'update_entries',\n",
       " 'update_entry',\n",
       " 'use_iterator']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(index_to_crack.invlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think Horizontal Slice merging is what we want with InvLists to increase number of inverted lists. At least based on this: https://github.com/facebookresearch/faiss/wiki/Indexing-1T-vectors#split-across-inverted-lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use combined index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use VStack to stack invlists in this manner, you need to take care not to add query point as point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans... nlist=1 km_n_iter=0 km_max_pts=1 seed=1 nredo=1\n",
      "Training level-1 quantizer\n",
      "Training level-1 quantizer on 1 vectors in 128D\n",
      "Training IVF residual\n",
      "IndexIVF: no residual training\n",
      "IndexIVFFlat::add_core: added 1 / 1 vectors\n",
      "\t---> Index Train Time = 0.4050589996040799 ms | Add Time = 1.154796002083458 ms <---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1 points to 1 centroids: please provide at least 39 training points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new index contains only xq (centroid xq invlists xq etc.)\n",
    "new_index, train_time, add_time = train_ivfflat(\n",
    "    xq[0:1,:],\n",
    "    nlist=1,\n",
    "    km_n_iter=0,\n",
    "    km_max_pts=1,\n",
    "    seed=1,\n",
    "    store_dir=None,  # if you want to store the index\n",
    "    verbose=True,\n",
    "    metric=metric,\n",
    ")\n",
    "new_index.remove_ids(np.array([0])) # take care not to have query in the xb of this new index\n",
    "new_index.ntotal\n",
    "# new_index.quantizer.reconstruct_n()\n",
    "# xq[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = [index_to_crack, new_index]\n",
    "indexes[0].nlist\n",
    "indexes[1].nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ilv = faiss.InvertedListsPtrVector()\n",
    "for indx in indexes:\n",
    "    indx.nlist\n",
    "    il = faiss.extract_index_ivf(indx).invlists\n",
    "    ilv.push_back(il)\n",
    "big_il = faiss.VStackInvertedLists(ilv.size(), ilv.data())\n",
    "# big_il = faiss.HStackInvertedLists(ilv.size(), ilv.data())\n",
    "big_il.nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.VStackInvertedLists; proxy of <Swig Object of type 'faiss::VStackInvertedLists *' at 0x7fba9d355920> >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_il\n",
    "ntotal_new = big_il.compute_ntotal()\n",
    "ntotal_new  # take care not to have query in xb of new index\n",
    "assert ntotal_new == index_to_crack.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SKIP\n",
    "# big_il # invlists object, we still have to replace old index somehow to have this one?\n",
    "# index_to_crack.nlist = big_il.nlist # do pass assert in the next one\n",
    "# index_to_crack.replace_invlists(big_il, False) # TODO : Check that this works as intended..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see quantizer was not replaced, only invlists so this will probably lead to issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack.quantizer.reconstruct_n().shape \n",
    "index_to_crack.invlists.nlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_centroids = np.vstack([index_to_crack.quantizer.reconstruct_n(), new_index.quantizer.reconstruct_n()]) # \n",
    "new_centroids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (SKIP) replace quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_centroids = np.vstack([outer_centroids, first_query])\n",
    "if metric == \"euclidean\":\n",
    "    new_quantizer = faiss.IndexFlatL2(d)\n",
    "elif metric == \"angular\":\n",
    "    new_quantizer = faiss.IndexFlatIP(d)\n",
    "new_quantizer.add(new_centroids)\n",
    "_ = replace_ivf_quantizer(index_to_crack, new_quantizer) # replace quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it seems like it worked, but what about the centroid/index ids what are they now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 128)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack.quantizer.reconstruct_n().shape \n",
    "index_to_crack.invlists.nlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    " 'add_entries',\n",
    " 'add_entry',\n",
    " 'code_size',\n",
    " 'compute_ntotal',\n",
    " 'copy_subset_to',\n",
    " 'get_codes',\n",
    " 'get_ids',\n",
    " 'get_iterator',\n",
    " 'get_single_code',\n",
    " 'get_single_id',\n",
    " 'imbalance_factor',\n",
    " 'is_empty',\n",
    " 'list_size',\n",
    " 'merge_from',\n",
    " 'nlist',\n",
    " 'prefetch_lists',\n",
    " 'print_stats',\n",
    " 'release_codes',\n",
    " 'release_ids',\n",
    " 'reset',\n",
    " 'resize',\n",
    " 'this',\n",
    " 'thisown',\n",
    " 'update_entries',\n",
    " 'update_entry',\n",
    " 'use_iterator'\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't work, kernel crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "index_to_crack.invlists.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (CONTINUE) What if we create new index with specific quantizer and invlists as we set it? Will it then be correctly initialized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1001, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_nlist = big_il.nlist\n",
    "new_nlist\n",
    "new_centroids = np.vstack([index_to_crack.quantizer.reconstruct_n(), new_index.quantizer.reconstruct_n()]) # \n",
    "new_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7fba9d355d40> >"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list size in < 1: 1001 instances\n"
     ]
    }
   ],
   "source": [
    "new_quantizer = faiss.IndexFlatL2(d)\n",
    "new_quantizer\n",
    "new_quantizer.add(new_centroids)\n",
    "# new empty index\n",
    "new_index = faiss.IndexIVFFlat(new_quantizer, d, new_nlist, faiss.METRIC_L2)\n",
    "new_index.ntotal\n",
    "new_index.is_trained\n",
    "new_index.nlist\n",
    "new_index.invlists.nlist\n",
    "new_index.invlists.print_stats() # empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexIVF.replace_invlists of <faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fba9ca5de60> >>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.VStackInvertedLists; proxy of <Swig Object of type 'faiss::VStackInvertedLists *' at 0x7fba9d355920> >"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list size in < 1: 1 instances\n",
      "list size in < 32: 3 instances\n",
      "list size in < 64: 8 instances\n",
      "list size in < 128: 32 instances\n",
      "list size in < 256: 100 instances\n",
      "list size in < 512: 211 instances\n",
      "list size in < 1024: 288 instances\n",
      "list size in < 2048: 255 instances\n",
      "list size in < 4096: 90 instances\n",
      "list size in < 8192: 13 instances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index.replace_invlists\n",
    "big_il # invlists object, we still have to replace old index somehow to have this one?\n",
    "new_index.nlist = big_il.nlist # do pass assert in the next one\n",
    "new_index.replace_invlists(big_il, False) # True/False is wether own_invlists ( ie if index is deleted this arrayinvlists is deleted)\n",
    "new_index.own_invlists\n",
    "new_index.ntotal\n",
    "new_index.is_trained\n",
    "new_index.nlist\n",
    "new_index.invlists.nlist\n",
    "new_index.invlists.print_stats() # empty\n",
    "new_index.ntotal # why 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_il.compute_ntotal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntotal = big_il.compute_ntotal()\n",
    "new_index.ntotal = ntotal\n",
    "new_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fba9ca5de60> >"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.,  25.,  11.,   0.,  22.,  31.,   6.,   8.,  10.,   3.,   0.,\n",
       "          1.,  30.,  91.,  88.,  18.,  38.,  44.,  16.,   1.,   5.,  25.,\n",
       "         70.,  36.,   1.,  22.,  10.,   7.,  10.,  40.,  61.,   1.,  60.,\n",
       "          9.,   8.,   4., 111.,  45.,  21.,  45.,   9.,   1.,   1.,  14.,\n",
       "        111., 111.,  70.,  26.,  95.,  13.,   3.,   2.,   3.,  39., 111.,\n",
       "        111.,  20.,   3.,  11.,  11.,   1.,  32.,  70.,  22.,  48.,   8.,\n",
       "          9.,  25.,  60.,  26.,  14.,  37.,   4.,   5.,  65., 110., 111.,\n",
       "         31.,   1.,   0., 101.,  78.,  84.,  34.,   4.,   2.,   2.,  29.,\n",
       "         33.,  44.,  25.,  22.,   2.,   0.,   4.,  18.,  54.,  51.,  24.,\n",
       "         21.,  12.,  18.,   5.,   6.,  11.,  17., 100.,  65.,  50.,  92.,\n",
       "         37.,  14.,  23.,  77.,  95.,   9.,   3.,  14.,  60.,  40.,   4.,\n",
       "         30.,  23.,  32.,  10.,   3.,  19.,  13.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq[1:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[413247, 413071, 706838, 880592, 249062, 400194, 942339, 880462,\n",
       "        987636, 941776, 420802, 586780, 833054, 248426, 849742, 208391,\n",
       "        271353, 192537, 987215, 832927,  20845, 948375, 266890, 202612,\n",
       "        980641, 183334, 517923, 976547, 517238, 133524, 833335, 974985,\n",
       "        560219, 731804,  71819, 980027, 538702, 766416, 987696, 122309,\n",
       "        541805, 480949, 826670, 974930, 522078,  14836, 543219, 239335,\n",
       "        250786, 833086, 406020, 598353, 491737, 957556, 956542, 214501,\n",
       "        133522, 974481, 250884, 395932, 123544, 207435, 522482, 913887,\n",
       "        807790, 808402, 850854, 509041, 378239, 411149, 868337, 837940,\n",
       "        704973, 196686, 275502, 915764,  17970, 678724, 908803, 855295,\n",
       "        741996, 913942, 490751, 739835, 730127, 618740, 394982, 253083,\n",
       "        993635,   2781, 496660, 947005, 502126, 888719, 705714, 125095,\n",
       "        862007, 855176, 846198, 987074]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index.nprobe = 2\n",
    "qid = 1\n",
    "a, b = new_index.search(xq[qid:qid+1,:], k=100)\n",
    "gt[qid:qid+1,:]\n",
    "compute_recall(b, gt[qid:qid+1,:], k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexIVFFlat; proxy of <Swig Object of type 'faiss::IndexIVFFlat *' at 0x7fba9d354180> >"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[413247, 413071, 706838, 880592, 249062, 400194, 942339, 880462,\n",
       "        987636, 941776, 420802, 586780, 833054, 248426, 849742, 208391,\n",
       "        271353, 192537, 987215, 832927,  20845, 948375, 266890, 202612,\n",
       "        980641, 183334, 517923, 976547, 517238, 133524, 833335, 974985,\n",
       "        560219, 731804,  71819, 980027, 538702, 766416, 987696, 122309,\n",
       "        541805, 480949, 826670, 974930, 522078,  14836, 543219, 239335,\n",
       "        250786, 833086, 406020, 598353, 491737, 957556, 956542, 214501,\n",
       "        133522, 974481, 250884, 395932, 123544, 207435, 522482, 913887,\n",
       "        807790, 808402, 850854, 509041, 378239, 411149, 868337, 837940,\n",
       "        704973, 196686, 275502, 915764,  17970, 678724, 908803, 855295,\n",
       "        741996, 913942, 490751, 739835, 730127, 618740, 394982, 253083,\n",
       "        993635,   2781, 496660, 947005, 502126, 888719, 705714, 125095,\n",
       "        862007, 855176, 846198, 987074]], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_crack.nprobe = 2\n",
    "qid = 1\n",
    "a, b = index_to_crack.search(xq[qid:qid+1,:], k=100)\n",
    "gt[qid:qid+1,:]\n",
    "compute_recall(b, gt[qid:qid+1,:], k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above how to add a new centroid to invlist... But points were not re-assigned. So you need to remove_ids () or resize.   \n",
    "TBD adding this to cracking code, but this is how I can increase Q-means. I can seperate \"addings\" centroids with re-assigning them.\n",
    "There is a remove_ids() implementation that is more efficient than what I hade with external IDMAP() object. I should look into it.   \n",
    "There is also just .resize(0) and add preassigned all poitns again (which is what I am doing.)    \n",
    "But if there is not enough re-assignments, only few points change clusters, then we can use the first more optimized version    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- make sure previous indexes and created objects are deleted to avoid memory leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = faiss.IndexFlatL2(d)\n",
    "if metric == \"euclidean\":\n",
    "    new_index = faiss.IndexIVFFlat(quantizer, d, new_nlist, faiss.METRIC_L2)\n",
    "elif metric == \"angular\":\n",
    "    new_index = faiss.IndexIVFFlat(quantizer, d, new_nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "new_index.cp.seed = 42\n",
    "new_index.cp.niter = 0\n",
    "new_index.cp.max_points_per_centroid = 256\n",
    "new_index.cp.nredo = 1\n",
    "new_index.verbose = True\n",
    "\n",
    "strain = time.perf_counter()\n",
    "index.train(data)\n",
    "etrain = time.perf_counter()\n",
    "\n",
    "sadd = time.perf_counter()\n",
    "index.add(data)\n",
    "eadd = time.perf_counter()\n",
    "\n",
    "if store_dir is not None:\n",
    "    filename = (\n",
    "        store_dir\n",
    "        + f\"/index-n_iter_{km_n_iter}-nlist_{nlist}-max_pts_{km_max_pts}-seed_{seed}.index\"\n",
    "    )\n",
    "    if args.store:\n",
    "        print(\"storing index\", filename)\n",
    "        faiss.write_index(index, filename)\n",
    "\n",
    "print(f\"\\t---> Index Train Time = {(etrain - strain)*1000} ms | Add Time = {(eadd - sadd)*1000} ms <---\")\n",
    "return index, etrain - strain, eadd - sadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check from the following how to do above (maybe it's here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 177\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     ci \u001b[38;5;241m=\u001b[39m \u001b[43mCombinedIndexDeep1B\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded index of size \u001b[39m\u001b[38;5;124m'\u001b[39m, ci\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mntotal)\n\u001b[1;32m    180\u001b[0m     deep1bdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets01_101/simsearch/041218/deep1b/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[70], line 162\u001b[0m, in \u001b[0;36mCombinedIndexDeep1B.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m masked_index_fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m invlist_fnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/hslices/slice\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.faissindex\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (workdir, i)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    161\u001b[0m ]\n\u001b[0;32m--> 162\u001b[0m \u001b[43mCombinedIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvlist_fnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_index_fname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 33\u001b[0m, in \u001b[0;36mCombinedIndex.__init__\u001b[0;34m(self, invlist_fnames, empty_index_fname, masked_index_fname)\u001b[0m\n\u001b[1;32m     31\u001b[0m         il \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mextract_index_ivf(index)\u001b[38;5;241m.\u001b[39minvlists\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     ilv\u001b[38;5;241m.\u001b[39mpush_back(il)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CombinedIndex:\n",
    "    \"\"\"\n",
    "    combines a set of inverted lists into a hstack\n",
    "    masks part of those lists\n",
    "    adds these inverted lists to an empty index that contains\n",
    "    the info on how to perform searches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, invlist_fnames, empty_index_fname,\n",
    "                 masked_index_fname=None):\n",
    "\n",
    "        self.indexes = indexes = []\n",
    "        ilv = faiss.InvertedListsPtrVector()\n",
    "\n",
    "        for fname in invlist_fnames:\n",
    "            if os.path.exists(fname):\n",
    "                print('reading', fname, end='\\r', flush=True)\n",
    "                index = faiss.read_index(fname)\n",
    "                indexes.append(index)\n",
    "                il = faiss.extract_index_ivf(index).invlists\n",
    "            else:\n",
    "                raise AssertionError\n",
    "            ilv.push_back(il)\n",
    "        print()\n",
    "\n",
    "        self.big_il = faiss.VStackInvertedLists(ilv.size(), ilv.data())\n",
    "        if masked_index_fname:\n",
    "            self.big_il_base = self.big_il\n",
    "            print('loading', masked_index_fname)\n",
    "            self.masked_index = faiss.read_index(\n",
    "                masked_index_fname,\n",
    "                faiss.IO_FLAG_MMAP | faiss.IO_FLAG_READ_ONLY)\n",
    "            self.big_il = faiss.MaskedInvertedLists(\n",
    "                faiss.extract_index_ivf(self.masked_index).invlists,\n",
    "                self.big_il_base)\n",
    "\n",
    "        print('loading empty index', empty_index_fname)\n",
    "        self.index = faiss.read_index(empty_index_fname)\n",
    "        ntotal = self.big_il.compute_ntotal()\n",
    "\n",
    "        print('replace invlists')\n",
    "        index_ivf = faiss.extract_index_ivf(self.index) # create a new empty index\n",
    "        index_ivf.replace_invlists(self.big_il, False) # replace the invlists\n",
    "        index_ivf.ntotal = self.index.ntotal = ntotal # the new ntotal\n",
    "        index_ivf.parallel_mode = 1   # seems reasonable to do this all the time << CHEF: not sure what this is\n",
    "\n",
    "        quantizer = faiss.downcast_index(index_ivf.quantizer)\n",
    "        quantizer.hnsw.efSearch = 1024 # chef: also not sure what this is\n",
    "\n",
    "    ############################################################\n",
    "    # Expose fields and functions of the index as methods so that they\n",
    "    # can be called by RPC\n",
    "\n",
    "    def search(self, x, k):\n",
    "        return self.index.search(x, k)\n",
    "\n",
    "    def range_search(self, x, radius):\n",
    "        return self.index.range_search(x, radius)\n",
    "\n",
    "    def transform_and_assign(self, xq):\n",
    "        index = self.index\n",
    "\n",
    "        if isinstance(index, faiss.IndexPreTransform):\n",
    "            assert index.chain.size() == 1\n",
    "            vt = index.chain.at(0)\n",
    "            xq = vt.apply_py(xq)\n",
    "\n",
    "        # perform quantization\n",
    "        index_ivf = faiss.extract_index_ivf(index)\n",
    "        quantizer = index_ivf.quantizer\n",
    "        coarse_dis, list_nos = quantizer.search(xq, index_ivf.nprobe)\n",
    "        return xq, list_nos, coarse_dis\n",
    "\n",
    "\n",
    "    def ivf_search_preassigned(self, xq, list_nos, coarse_dis, k):\n",
    "        index_ivf = faiss.extract_index_ivf(self.index)\n",
    "        n, d = xq.shape\n",
    "        assert d == index_ivf.d\n",
    "        n2, d2 = list_nos.shape\n",
    "        assert list_nos.shape == coarse_dis.shape\n",
    "        assert n2 == n\n",
    "        assert d2 == index_ivf.nprobe\n",
    "        D = np.empty((n, k), dtype='float32')\n",
    "        I = np.empty((n, k), dtype='int64')\n",
    "        index_ivf.search_preassigned(\n",
    "            n, faiss.swig_ptr(xq), k,\n",
    "            faiss.swig_ptr(list_nos), faiss.swig_ptr(coarse_dis),\n",
    "            faiss.swig_ptr(D), faiss.swig_ptr(I), False)\n",
    "        return D, I\n",
    "\n",
    "\n",
    "    def ivf_range_search_preassigned(self, xq, list_nos, coarse_dis, radius):\n",
    "        index_ivf = faiss.extract_index_ivf(self.index)\n",
    "        n, d = xq.shape\n",
    "        assert d == index_ivf.d\n",
    "        n2, d2 = list_nos.shape\n",
    "        assert list_nos.shape == coarse_dis.shape\n",
    "        assert n2 == n\n",
    "        assert d2 == index_ivf.nprobe\n",
    "        res = faiss.RangeSearchResult(n)\n",
    "\n",
    "        index_ivf.range_search_preassigned(\n",
    "            n, faiss.swig_ptr(xq), radius,\n",
    "            faiss.swig_ptr(list_nos), faiss.swig_ptr(coarse_dis),\n",
    "            res)\n",
    "\n",
    "        lims = faiss.rev_swig_ptr(res.lims, n + 1).copy()\n",
    "        nd = int(lims[-1])\n",
    "        D = faiss.rev_swig_ptr(res.distances, nd).copy()\n",
    "        I = faiss.rev_swig_ptr(res.labels, nd).copy()\n",
    "        return lims, D, I\n",
    "\n",
    "    def set_nprobe(self, nprobe):\n",
    "        # CHEF: extract_index_ivf : get an IndexIVF from an index. The index may be an IndexIVF or some wrapper class that encloses an IndexIVF\n",
    "        index_ivf = faiss.extract_index_ivf(self.index)\n",
    "        index_ivf.nprobe = nprobe\n",
    "\n",
    "    def set_parallel_mode(self, pm):\n",
    "        index_ivf = faiss.extract_index_ivf(self.index)\n",
    "        index_ivf.parallel_mode = pm\n",
    "\n",
    "    def get_ntotal(self):\n",
    "        return self.index.ntotal\n",
    "\n",
    "    def set_prefetch_nthread(self, nt):\n",
    "        for idx in self.indexes:\n",
    "            il = faiss.downcast_InvertedLists(\n",
    "                faiss.extract_index_ivf(idx).invlists)\n",
    "            il.prefetch_nthread\n",
    "            il.prefetch_nthread = nt\n",
    "\n",
    "    def set_omp_num_threads(self, nt):\n",
    "        faiss.omp_set_num_threads(nt)\n",
    "\n",
    "class CombinedIndexDeep1B(CombinedIndex):\n",
    "    \"\"\" loads a CombinedIndex with the data from the big photodna index \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # set some paths\n",
    "        workdir = \"/checkpoint/matthijs/ondisk_distributed/\"\n",
    "\n",
    "        # empty index with the proper quantizer\n",
    "        indexfname = workdir + 'trained.faissindex'\n",
    "\n",
    "        # index that has some invlists that override the big one\n",
    "        masked_index_fname = None\n",
    "        invlist_fnames = [\n",
    "            '%s/hslices/slice%d.faissindex' % (workdir, i)\n",
    "            for i in range(50)\n",
    "        ]\n",
    "        CombinedIndex.__init__(self, invlist_fnames, indexfname, masked_index_fname)\n",
    "\n",
    "\n",
    "def ivecs_read(fname):\n",
    "    a = np.fromfile(fname, dtype='int32')\n",
    "    d = a[0]\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy()\n",
    "\n",
    "\n",
    "def fvecs_read(fname):\n",
    "    return ivecs_read(fname).view('float32')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    ci = CombinedIndexDeep1B()\n",
    "    print('loaded index of size ', ci.index.ntotal)\n",
    "\n",
    "    deep1bdir = \"/datasets01_101/simsearch/041218/deep1b/\"\n",
    "\n",
    "    xq = fvecs_read(deep1bdir + \"deep1B_queries.fvecs\")\n",
    "    gt_fname = deep1bdir + \"deep1B_groundtruth.ivecs\"\n",
    "    gt = ivecs_read(gt_fname)\n",
    "\n",
    "    for nprobe in 1, 10, 100, 1000:\n",
    "        ci.set_nprobe(nprobe)\n",
    "        t0 = time.time()\n",
    "        D, I = ci.search(xq, 100)\n",
    "        t1 = time.time()\n",
    "        print('nprobe=%d 1-recall@1=%.4f t=%.2fs' % (\n",
    "            nprobe, (I[:, 0] == gt[:, 0]).sum() / len(xq),\n",
    "            t1 - t0\n",
    "        ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
